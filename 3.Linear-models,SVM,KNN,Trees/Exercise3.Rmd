---
title: "Aprendizaje Automático"
subtitle: "Práctica 3"
author: "Braulio Vargas López"
date: "12 de mayo de 2016"
lang: es
header-includes:
  - \usepackage{enumerate}
  - \usepackage{hyperref}
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("ISLR")
library("ggplot2")
library("class")
library("e1071")
library("ROCR")
library("gbm")
library("boot")
library("glmnet")
library("MASS")
library("randomForest")
library("tree")
```

# Ejercicio 1
\sffamily\bfseries
Usar el conjunto de datos Auto que es parte del paquete ISLR.

En este ejercicio desarrollaremos un modelo para predecir si un coche tiene un consumo de carburante alto o bajo usando la base de datos Auto. Se considerará alto cuando sea superior a la mediana de la variable mpg y bajo en caso contrario.

\begin{enumerate}[a)]
  \item Usar las funciones de R \texttt{pairs()} y \texttt{boxplot()} para investigar la dependencia entre \textit{mpg} y las otras características. ¿Cuáles de las otras características parece más útil para predecir \textit{mpg}? Justificar la respuesta. 
  \item Seleccionar las variables predictoras que considere más relevantes.
  \item Particionar el conjunto de datos en un conjunto de entrenamiento (80\%) y otro de test (20\%). Justificar el procedimiento usado.
  \item Crear una variable binaria, $mpg01$, que será igual 1 si la variable $mpg$ contiene un valor por encima de la mediana, y -1 si $mpg$ contiene un valor por debajo de la mediana. La mediana se puede calcular usando la función \texttt{median()}. (Nota: puede resultar útil usar la función \texttt{data.frame()} para unir en un mismo conjunto de datos la nueva variable $mpg01$ y las otras variables de Auto).
  \begin{enumerate}[$\bullet$]
    \item  Ajustar un modelo de Regresión Logística a los datos de entrenamiento y predecir $mpg01$ usando las variables seleccionadas en b). ¿Cuál es el error de test del modelo? Justificar la respuesta.
    \item  Ajustar un modelo K-NN a los datos de entrenamiento y predecir $mpg01$ usando solamente las variables seleccionadas en b). ¿Cuál es el error de test en el modelo? ¿Cuál es el valor de K que mejor ajusta los datos? Justificar la respuesta. (Usar el paquete class de R).
    \item Pintar las curvas ROC (instalar paquete ROCR en R) y comparar y valorar los resultados obtenidos para ambos modelos.
  \end{enumerate}
  \item Estimar el error de test de ambos modelos (RL, K-NN) pero usando Validación Cruzada de 5-particiones. Comparar con los resultados obtenidos en el punto anterior.
  \item Ajustar el mejor modelo de regresión posible considerando la variable mpg como salida y el resto como predictoras. Justificar el modelo ajustado en base al patrón de los residuos. Estimar su error de entrenamiento y test.
\end{enumerate}
\normalfont

## Solución

a) En este apartado, podemos mostrar las gráficas resultantes de cruzar todos con todos, y ver en cuales de ellas parece haber alguna relación. Para ello, usamos lo siguiente:

```{r}
attach(Auto)
pairs(~ ., data=Auto)
```

En estas gráficas, vemos que existe una relación entre $mpg$ y $displacement$, $mpg$ y $horsepower$ y entre $mpg$ y $weight$, ya que se ve una relación clara entre estas variables, mientras que en otras, podemos ver que los datos varían demasiado o tienen mucho ruido como para poder aprender algo.

Si usamos \texttt{boxplot} para representar las variables obtenemos las siguientes gráficas:

```{r}
boxplot(mpg ~ displacement, data=Auto, xlab="Mpg", ylab = "Displacement")
boxplot(mpg ~ horsepower, data=Auto, xlab="Mpg", ylab = "Horsepower")
boxplot(mpg ~ weight, data=Auto, xlab="Mpg", ylab = "Weight")
boxplot(mpg ~ acceleration, data=Auto, xlab="Mpg", ylab = "Acceleration")
```


En las distintas gráficas, podemos ver la relación que hay entre los dos elementos que estamos comparando. En esta relación podemos ver representados cómo la ``caja'' representa el intervalo entre el cuartil 1 y el 3, y el rango de valores total. Además, los $outliers$ quedan representados como un solo punto. En las distintas gráficas, podemos ver como todas tienen una varianza pequeña, o el ruido en la función es menor, excepto si vemos el resultado de \texttt{boxplot}entre $mpg$ y $acceleration$, que vemos como se ve incrementado la varianza y el ruido en los datos.

b) Una vez visto lo anterior, las variables más relevantes para poder predecir $mpg$ vemos que son \textit{displacement}, \textit{horsepower} y \textit{weight}. Pero, para escoger mejor los datos, podemos hacer un ajuste lineal rápido con la función \texttt{lm} y ver qué variable de estas tres es más importante con la función \texttt{summary}.

c) Para dividir el conjunto de datos en entrenamiento y test, podemos hacer lo siguiente:

```{r}
i = sample(x=nrow(Auto), size=floor(nrow(Auto))*0.8)
data.train = Auto[i,]
data.test = Auto[-i,]
```

Para dividir los datos, cogemos una muestra aleatoria de índices de longitud igual $0,8 \cdot n_{muestras}$, entre 1 y el número total de muestras que tenemos. Esta muestra de índices, la usaremos para aprender el modelo con el 80\% de los datos, y usar el 20\% para test.

Una vez divididos, vamosa realizar una serie de modelos lineales, para estudiar cuál de ellos es mejor:

1) En este caso, realizamos un modelo lineal simple que tiene en cuenta solo $mpg$ en función de $weight$
```{r}
i = sample(x=nrow(Auto), size=floor(nrow(Auto))*0.8)
m01= lm(mpg~weight,data = Auto, subset= i)
summary(m01)
(ggplot() + geom_point(data=as.data.frame(data.test), 
            aes(x=data.test$weight, y=data.test$mpg)) 
          + geom_line(aes(x=data.test$weight,
                          y=predict(m01,data.test), color="red")))

```
En el summary, podemos ver cómo el error dentro de la muestra, es del 0.6\%, que para ser un modelo lineal simple, no está nada mal. Pero, al estimar los valores con el conjunto de test, vemos que tampoco lo hace mal, pero se puede mejorar y para ello, tendremos los siguientes modelos.

2) En este caso, ya no solo tenemos en cuenta $weight$, si no que ya tenemos en cuenta también $displacement$ y $horsepower$. En este caso, obtenemos que el error es de casi el 0.7\%, un poco mayor que en el modelo anterior, pero, fuera de la muestra, lo hace bastante mejor que $m01$.
```{r}
m02 = lm(mpg ~ weight+displacement+horsepower, data=Auto, subset = i)
summary(m02)
(ggplot() + geom_point(data=as.data.frame(data.test), 
            aes(x=data.test$horsepower*data.test$displacement*data.test$weight,
                y=data.test$mpg)) 
          + geom_line(aes(x=data.test$horsepower*data.test$displacement*data.test$weight, 
                          y=predict(m02,data.test), color="red")))
```
3) En este modelo, comenzamos a usar modelos con funciones cuadráticas, para calcular el valor del punto, usando $weight$ y el cuadrado de $weight$ y $horsepower$. En este modelo, \texttt{summary} nos indica que las características que usamos para calcular el valor del punto están muy relacionadas y son importantes para el cálculo. Además, aunque el error que obtenemosd dentro de la muestra es mayor que el de los anteriores, 0.7, vemos que el comportamiento fuera de la muestra es mucho mejor que el de los anteriores.
```{r}
m03 = lm(mpg~weight*I(horsepower+displacement)^2,data = Auto, subset= i)
summary(m03)
(ggplot() + geom_point(data=as.data.frame(data.test), 
            aes(x=data.test$weight*I((data.test$horsepower+data.test$displacement)^2), 
                y=data.test$mpg)) 
          + geom_line(aes(x=data.test$weight*I((data.test$horsepower+
                data.test$displacement)^2), 
                y=predict(m03,data.test), color="red")))
```
4) En este último modelo, realizamos un modelo también no lineal de orden 2, relacionando las tres características entre sí y a su vez, con $weight$. Como primera mejora obtenemos que el error dentro de la muestra respecto al modelo anterior se ha reducido. La segunda mejora es que fuera de la muestra, lo hace incluso mejor, como se ve en la gráfica de la función y el ajuste del modelo.
```{r}
m04 = lm(mpg~weight*I(horsepower*displacement*weight)^2,data = Auto, subset= i)
summary(m04)
(ggplot() + geom_point(data=as.data.frame(data.test), 
            aes(x=data.test$weight*I(data.test$horsepower*
              data.test$displacement*data.test$weight)^2, 
              y=data.test$mpg)) 
          + geom_line(aes(x=data.test$weight*I(data.test$horsepower*
              data.test$displacement*data.test$weight)^2, 
              y=predict(m04,data.test), color="red")))
```
\label{m04}
d) Para crear la variable $mpg01$, podemos hacer uso de la función $sapply$, para asignar un uno o un cero en función de si el valor del dato $mpg_i$ está por encima o por debajo de la mediana de $mpg$.

```{r}
# Calculamos la media de mpg
medianMPG = median(mpg)
# Y etiquetamos los datos con valores 0/1 en función de si son
# mayores o menores que la mediana de mpg
mpg01 = (mpg >= medianMPG)*1
# Y creamos un dataframe con estos datos, eliminando
# los nombres del dataframe. También generamos un
# dataframe con los datos de test
datos = data.frame(Auto,as.factor(mpg01))
colnames(datos)[ncol(datos)]="mpg01"
datos$name = NULL
datos.test = data.frame(datos[-i,])
```

* Para ajustar la regresión logística, hacemos uso de $glm$, con $mpg$ en función de las variables $weight$, $displacement$ y $horsepower$. Crearemos varios modelos en función con regresión logística, teniendo en cuenta varios parámetros. 

En este primer modelo, realizamos la regresión logística de $mpg$ en función de $weight$. Una vez aprendido el modelo, calculamos las etiquetas con el modelo aprendido y generamos la matriz de confusión. Tras esto, calculamos el error dentro de la muestra ($E_{in}$) y fuera de la muestra ($E_{out}$) con los datos 
de test.
```{r}
# Calculamos la regresión logística
attach(datos)
glmModelErrorTest <- function(glm_model, trainingIndex, data, testData, getPredicts = F) {
  # Hacemos la predicción con los datos de test
  glm.prediction.model.train = predict(glm_model, type="response")
  glm.prediction.model.test = predict(object=glm_model, data.frame(testData), type="response")
  # Y calculamos las etiquetas con predict
  glm.prediction.model.train.y = glm.prediction.model.train
  glm.prediction.model.train.y[glm.prediction.model.train.y <= 0.5] = 0
  glm.prediction.model.train.y[glm.prediction.model.train.y > 0.5] = 1
  glm.prediction.model.test.y = glm.prediction.model.test
  glm.prediction.model.test.y[glm.prediction.model.test.y <= 0.5] = 0
  glm.prediction.model.test.y[glm.prediction.model.test.y > 0.5] = 1
  # Generamos la matriz de confusión con los datos de test
  print(table(glm.prediction.model.train.y, data$mpg01[trainingIndex]))
  print(table(glm.prediction.model.test.y, testData$mpg01))
  if(!getPredicts){
    cat("Ein: ", mean(glm.prediction.model.train.y != mpg01[trainingIndex]) ,"\n")
    cat("Eout: ", mean(glm.prediction.model.test.y != testData$mpg01) ,"\n")
  }
  else
    list(glm.prediction.model.test, glm.prediction.model.train)
}

glm.model1 = glm(formula = mpg01 ~ weight, 
     data = datos, subset = i, family = "binomial")

glmModelErrorTest(glm.model1, i, datos, datos.test)
```

En este segundo modelo, realizamos lo mismo, pero con $mpg$ en función de $horsepower$.

```{r}
glm.model2 = glm(mpg01 ~ horsepower, data = datos,
   subset = i, family = "binomial")

glmModelErrorTest(glm.model2, i, datos, datos.test)
```

Como tercer modelo, tenemos la regresión logística de $mpg$ en función de $displacement$.

```{r}
glm.model3 = glm(mpg01 ~ displacement, data = datos,
  subset = i, family = "binomial")
glmModelErrorTest(glm.model3, i, datos, datos.test)
```

En este modelo, realizamos algo más complejo, donde calculamos la regresión logística de $mpg$ en función de $horsepower$, $displacement$ y $weight$, estando relacionados los tres. Tras esto, al igual que en los anteriores, obtenemos las matrices de confusión, y calculamos el error dentro y fuera de la muestra.

```{r}
glm.model4 = glm(mpg01 ~ weight*horsepower*displacement, 
  data = datos, subset = i, family = "binomial")
glmModelErrorTest(glm.model4, i, datos, datos.test)
```

En estos dos últimos modelos, realizamos una regresión logística con un modelo cuadrático, ajustando los datos. Una vez aprendido, predecimos con los datos de test, y calculamos el error dentor y fuera de la muestra. Estos dos modelos son los que calculamos  en el apartado anterior:

```{r}
glm.model5 = glm(mpg01 ~ weight*I(horsepower+displacement)^2, 
  data = datos, subset = i, family="binomial")
glmModelErrorTest(glm.model5, i, datos, datos.test)
```

En este caso, vemos que el modelo aprendido resulta ser un poco mejor que el modelo anterior, ya que, aunque el error dentro de la muestra sea peor en este modelo, vemos que el error fuera de la muestra mejora al modelo anterior. Y por último, vamos a probar con el último modelo que realizamos en el apartado anterior.

```{r}
glm.model6 = glm(mpg01 ~ weight*I(horsepower*displacement*weight)^2, 
  data = datos, subset = i, family = "binomial")
glmModelErrorTest(glm.model6, i, datos, datos.test)
```

Como vemos, el error dentro de la muestra en este caso viene a ser un poco peor que el modelo anterior, pero fuera de la muestra, se comportan más o menos igual, diferenciandose en el número de falsos positivos y falsos negativos, pero obteniendo el mismo error.

Por lo tanto, los mejores modelos son $glm.model5$ y $glm.model6$ ya que obtienen los errores más pequeños fuera de la muestra. Y, al elegir entre estos dos, cogeremos $glm.model5$, ya que al ser el modelo más simple tiene preferencia si usamos el criterio de la *navaja de Ockham*.


* Ahora vamos a proceder a utlizar el $KNN$ para ajustar el modelo. La función \texttt{knn} de la biblioteca $class$, realiza directamente la predicción de los datos, a diferencia de los modelos lineales anteriores. Para ello, la función toma como valores de entrada el conjunto de training, de test, las etiquetas del conjunto de training y el $k$, que inicialmente será $k=1$.

Además de esto, para que el $KNN$ funcione correctamente, tenemos que normalizar los datos, para que las dimensiones de uno no condicionen el resultado, al ser distintas.

```{r}
set.seed(1)

normalize <- function(data) {
    apply(X=data, MARGIN=2, FUN=function(x) {
        max <- max(x)
        min <- min(x)
        sapply(X=x, FUN=function(xi) (xi-min)/(max-min))
    })
}
# normalizamos los datos

caracteristicas = data.frame(horsepower, weight, displacement)
names(caracteristicas) = c("horsepower", "weight", "displacement")

vs_train = data.frame(data.frame(normalize(caracteristicas[i,])), as.factor(mpg01[i]))
colnames(vs_train) = c("horsepower", "weight", "displacement", "mpg01")
vs_test = data.frame(data.frame(normalize(caracteristicas[-i,])), datos.test$mpg01)
colnames(vs_test) = c("horsepower", "weight", "displacement", "mpg01")

# esta función clasifica los datos con KNN
learnKNN <- function(train, test, label, testLabel, k, pintarM = T, useProb=F){
   knn.model=knn(train, test, label, k = k, prob = useProb)
   if(pintarM) print(table(knn.model, testLabel))
     knn.model
   }
knn1=learnKNN(train=subset(vs_train, select=-mpg01) , test=subset(vs_test, select=-mpg01),
  vs_train$mpg01, vs_test$mpg01, k = 1)
error1 = mean(knn1 != datos.test$mpg01)
cat("Eout: ", error1, "\n")
```

Como vemos, aproximadamente el 90\% de las muestras están bien etiquetadas. Para ver si se mejora o no, vamos a cambiar el tamaño de $k$. Esto lo haremos con la función \texttt{tune.knn}, la cual, nos devolverá el modelo con el mejor $k$ y el mejor $k$.

```{r}
model.tune.knn = tune.knn(x=subset(vs_train, select=-mpg01), y=vs_train$mpg01, k=1:20)
print(model.tune.knn)
```
 
En el resultado de la función, podemos ver cómo la función después de ajustar el modelo con los distintos $k$ que puede haber en el intervalo que hemos establecido, escoge el $k$ que ajusta mejor al modelo.

* Para pintar las curvas ROC, haremos uso del paquete $ROCR$ de la siguiente manera, como se puede ver en el libro ``\textit{An Introduction to Statistical Learning}'':

```{r}
rocplot <- function(model, test, Add_plot = F, colour = "red"){
  y_pred = prediction(model, test)
  perf = performance(y_pred, "tpr", "fpr")
  
  plot(perf, add = Add_plot, col = colour)
}
pred=glmModelErrorTest(glm.model6, i, datos, datos.test, getPredicts = T)[1]
rocplot(model = pred, test = datos.test$mpg01)
# Curva ROC para KNN
bestKNN=learnKNN(train=subset(vs_train, select=-mpg01) , test=subset(vs_test, select=-mpg01),
  vs_train$mpg01, vs_test$mpg01, k = model.tune.knn$best.model$k, useProb = T, pintarM = F)
probKNN = attr(bestKNN, "prob")
probKNN = ifelse(bestKNN == 0, 1 - probKNN, probKNN)
rocplot(model = probKNN, test = datos.test$mpg01, Add_plot = T, colour = "blue")
legend('bottomright', c("GLM","KNN"), col=c('red', 'blue'), lwd=3)
```

Como podemos ver en el resultado de la ejecución, la curva ROC de $KNN$ es un poco mejor que la de $GLM$, con lo que obtenemos que el modelo KNN es un poco mejor que regresión para este conjunto de datos, ya que ajusta mejor los datos y clasifica mejor que $GLM$.

* Para estimar el error usando validación cruzada, usaremos la librería \texttt{boot}. Como $k=5$, haremos un bucle de 1 a 5, e iremos almacenando los errores de CV.

```{r}
attach(Auto)
lm.fit = glm(mpg01 ~ weight*I(horsepower*displacement*weight)^2, 
  data = datos, family = "binomial")
cv.error.5 = cv.glm(data = datos, glmfit=lm.fit, K=5)$delta[1]

print(cv.error.5)
```

Como vemos, estos son los errores que podemos calcular con validación cruzada usando regresión lineal, y con el cuarto modelo de regresión calculado. A continuación, podemos ver los errores de CV para el KNN: 

```{r}
model.tune.knn = tune.knn(x=subset(vs_train, select=-mpg01), 
    y=vs_train$mpg01, k=1:20, tunecontrol=tune.control(cross=5))
print(model.tune.knn)
```

* En este caso, el mejor modelo obtenido es el modelo $KNN$ donde hemos podido comprobar a lo largo del ejercicio, como ajusta mejor los datos y obtiene mejores resultados que el resto. Además, podemos comprobarlo facilmente en la curva ROC.

# Ejercicio 2
\sffamily\bfseries
Usar la base de datos Boston (en el paquete MASS de R) para ajustar un modelo que prediga si dado un suburbio este tiene una tasa de criminalidad (crim) por encima o por debajo de la mediana. Para ello considere la variable crim como la variable salida y el resto como variables predictoras. 

a) Encontrar el subconjunto óptimo de variables predictoras a partir de un modelo de regresión-LASSO (usar paquete glmnet de R) donde seleccionamos solo aquellas variables con coeficiente mayor de un umbral prefijado. 
b) Ajustar un modelo de regresión regularizada con “weight-decay” (ridge-regression) y las variables seleccionadas. Estimar el error residual del modelo y discutir si el comportamiento de los residuos muestran algún indicio de “underfitting”.
c) Definir una nueva variable con valores -1 y 1 usando el valor de la mediana de la variable crim como umbral. Ajustar un modelo SVM que prediga la nueva variable definida. (Usar el paquete e1071 de R). Describir con detalle cada uno de los pasos dados en el aprendizaje del modelo SVM. Comience ajustando un modelo lineal y argumente si considera necesario usar algún núcleo. Valorar los resultados del uso de distintos núcleos. 

Bonus-3: Estimar el error de entrenamiento y test por validación cruzada de 5 particiones.

\normalfont

a) Para realizar la selección del subconjunto ótpimo, haremos uso de la función \texttt{glmnet}, con el parámetro \texttt{alpha = 1}.

```{r}
grid =10^seq(10, -2, length=length(Boston$crim))

datos = model.matrix(Boston$crim ~., Boston)[,-1]
y_datos = as.vector(Boston$crim)
# indices para los datos de train
i = sample(x=nrow(datos), size=floor(nrow(datos))*0.8)

lasso.mod = glmnet(datos[i,], y_datos[i], alpha = 1, lambda = grid)
plot(lasso.mod)
legend('bottomleft', rownames(lasso.mod$beta), 
  col=rainbow(n=length(rownames(lasso.mod$beta))), lwd=2, ncol = 3)
cv.out = cv.glmnet(datos, y_datos, alpha = 1)
plot(cv.out)
mejorLambda = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = mejorLambda, newx=datos[-i,])
mean(lasso.pred - y_datos[-i])
out = glmnet(datos, y_datos, lambda = grid, alpha = 1)
lasso.coef = predict(out, type="coefficients", s = mejorLambda)
subgrupo = abs(lasso.coef[,1])
names(subgrupo[subgrupo >= 0.5])
```

Como vemos, el número de variables ha bastante reducido, quedando solo las más relevantes, dentro del umbral establecido, tras haber realizado un cálculo inicial, con los datos regularizados, para unos datos de training.

A continuación, obtenemos un $\lambda$ realizando validación cruzada para predecir los datos y vemos la media de errores cuadráticos en los datos de test. Por último, tras haber escogido el mejor $\lambda$ volvemos a llamar al método \texttt{glmnet} y realizamos la predicción con el mejor $\lambda$ para obtener los coeficientes para cada variable, con la opción \texttt{type = "coefficients"}, y nos quedamos con los que superen el umbral.

b) Para hacer la \textit{ridge-regression}, haremos uso de los datos de antes, ya preparados, gracias a la función \texttt{model.matrix}. 

```{r}
attach(Boston)
x = cbind(chas, nox, dis, rad)
ridge.mod = glmnet(x[i,], y_datos[i], alpha=0, lambda = grid)
ridge.pred = predict(ridge.mod, s = mejorLambda, newx=x[-i,])
dif = ridge.pred - y_datos[-i]
plot(dif)
mean(dif)
```

Como podemos ver en la gráfica, el modelo es capaz de ajustar bastate bien alrededor del 60\% de los datos de test, mientras que en el resto, vemos como hay indicios de \textit{underfitting} ya que se empiezan a producir errores en los datos, teniendo algunos errores considerables entre el modelo ajustado y el real, pero puede ser que el dato que ofrece el mayor error con diferencia a los demás, puede ser que sea un \textit{outlayer}.

c) Para hacer la nueva variable, procederemos de forma similiar a como se hacía en el ejercicio 1 para calcular $mpg01$. 
```{r}
crimMean = mean(crim)
newCrim = (crim >= crimMean)*1
newCrim[newCrim == 0] = -1
datos = data.frame(Boston,as.factor(newCrim))
colnames(datos)[ncol(datos)]="newCrim"
attach(datos)
SVM <- function(kernel = "linear", control = 10){
  # Realizamos la llamada al SVM
  svmfit = tune(svm, newCrim ~., data=datos, kernel = kernel,
      ranges = list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)),
      tunecontrol=tune.control(cross=control))
  svmfit.bestmodel = svmfit$best.model
  ys = predict=predict(svmfit.bestmodel, data.frame(datos[-i,]))
  print(table(predict=ys, truth=datos$newCrim[-i]))
  cat("CV error: ", svmfit$best.performance ,"\n")
  cat("Eout: ", mean(ys != datos[-i,]$newCrim) ,"\n")
}

SVM()
```

Como vemos, el ajuste que produce SVM es de una calidad muy superior a los modelos, ya que el modelo ajusta los datos según el kernel elegido, y prueba con la función \texttt{tune} los distintos rangos que aparecen en la lista del parámetro \texttt{ranges}. Para estimar el error del modelo, utiliza validación cruzada, y se queda con el modelo que obtiene mejor error, (almacenado en la variable \textit{best.perfomance}).

Tras realizar las pruebas con los distintos parámetros, escogemos el mejor modelo de todos los modelos posibles, con \textit{svmfit.bestmodel = svmfit\$best.model}. A continuación, mostramos la matriza de confusión y como podemos ver, los datos de la matriz nos indica que el error utilizando \textit{SVM} es muy pequeño, y al mostrar el error de validación cruzada en la muestra y el error fuera de la muestra, vemos que ajusta muy bien los datos.

En un principio, vemos que con el modelo lineal generaliza muy bien los datos y no solo eso, sino que los ajusta muy bien también para ser un kernel tan ``simple''. Pero podemos ver si cambiando de kernel, ajustamos mejor los datos.

```{r}
# KERNEL POLYNOMIAL
SVM(kernel="polynomial")
```

Como podemos ver, en un principio, para este set de datos, cambiar entre un kernel lineal y polinómico, no supone un cambio sustancial, por lo que podemos seguir eligiendo el kernel lineal.

```{r}
#KERNEL DE BASE RADIAL
SVM(kernel="radial")
```

```{r}
#KERNEL BASADO EN SIGMOIDES
SVM(kernel="sigmoid")
```

Como podemos ver, cambiar de kernel para este set de datos, con validación cruzada de 10 particiones, no suponen mejoría ante un kernel lineal, utilizando los parámetros por defecto, a excepción del kernel con base radial, que es capaz de ajustar un buen modelo también.

d) Para estimar el error de test y el error de validación cruzada con 5 particiones, podemos usar la misma función que en el apartado anterior, pero, cambiando el parámetro \texttt{tunecontrol}, haciendo que tome el valor 5, ya que por defecto realiza una partición de $K = 10$.

Para ello, en la función \textit{SVM} implementada, cambiaremos el valor del parámetro \texttt{control} a 5 para que se realice una validación cruzada con $k=5$.

```{r}
SVM(kernel="linear",control=5)
```

Como vemos, el error que se produce es mayor que si hacemos una validación cruzada con un $k$ más pequeño ya que el modelo entrena con menos datos y realiza las pruebas de test con conjuntos de datos más grandes, con lo que la probabilidad de error se hace mayor.